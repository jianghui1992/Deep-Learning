{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b766fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c4d1b",
   "metadata": {},
   "source": [
    "# data -准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "507db569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载MNIST训练数据集\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"../data/mnist\",               # 指定数据集下载和存储的根目录\n",
    "    train=True,                         # train=True表示加载训练集\n",
    "    transform=transforms.ToTensor(),    # 将图像数据转换为PyTorch张量，并将像素值归一化到[0,1]\n",
    "    download=True                       # 如果root目录下数据集不存在，则自动下载,存在就不会再次下载了\n",
    ")\n",
    "# 加载MNIST测试数据集\n",
    "test_data = datasets.MNIST(root=\"../data/mnist\",train=False,transform=transforms.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d01ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批次大小(batch_size)\n",
    "# - 将整个数据集分成多个小批次进行训练，而不是一次性使用所有数据\n",
    "# - 好处：1.减少内存占用 2.加快训练速度 3.引入随机性，有助于模型泛化\n",
    "# - 如果batch_size=1为随机梯度下降，如果等于数据集大小为全批量梯度下降,这里就是随机小批量梯度下降\n",
    "batch_size = 100\n",
    "\n",
    "# 创建训练数据加载器 shuffle=True: 数据打乱,每轮训练开始时,会将数据进行随机打乱,然后依次取出batch_size个样本\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 创建测试数据加载器  shuffle=False因为测试时不需要随机性 , 我们希望得到确定的、可复现的结果\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862ea5d",
   "metadata": {},
   "source": [
    "# net - 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f86e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 MLP 网络  继承nn.Module\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    # 初始化方法\n",
    "    # input_size输入数据的维度    \n",
    "    # hidden_size 隐藏层的大小\n",
    "    # num_classes 输出分类的数量\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        # 调用父类的初始化方法\n",
    "        super().__init__() \n",
    "        # 这里只是定义,顺序不影响,实际计算时会按照forward函数的顺序执行\n",
    "        # 定义第1个全连接层  \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # 定义激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 定义第2个全连接层\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 定义第3个全连接层\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    # 定义forward函数\n",
    "    # x 输入的数据\n",
    "    def forward(self, x):\n",
    "        # 网络结构: x->(fc1->relu)->(fc2->relu)->fc3->out\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        # 将第二层结果传递给fc3: 第二个隐藏层 -> 输出层\n",
    "        # 输出层不使用ReLU的原因:\n",
    "        # 1. 这是一个分类问题,最后一层需要得到每个类别的得分(logits)\n",
    "        # 2. 这些得分会被后续的CrossEntropyLoss处理,其内部包含了softmax激活函数\n",
    "        # 3. 如果在这里加ReLU会限制输出为非负数,影响模型的表达能力\n",
    "        out = self.fc3(out)\n",
    "        # 返回结果\n",
    "        return out\n",
    "    \n",
    "\n",
    "# 初始化MLP \n",
    "# - input_size为28*28是因为MNIST数据集的每张图片是28x28像素\n",
    "# - hidden_size可以根据需要调整，通常选择较大的值以增加模型的表达能力\n",
    "# - num_classes为10是因为MNIST数据集有10个数字类别（0-9）   \n",
    "model = MLP(input_size=28*28, hidden_size=512, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a1ae6",
   "metadata": {},
   "source": [
    "# loss -定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f7329fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵损失函数:适用于多分类问题\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd81a0b",
   "metadata": {},
   "source": [
    "# optim - 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1d15bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "# - 使用Adam优化器，它结合了动量(Momentum)和自适应学习率(RMSprop)的优点,是最常用的优化器之一\n",
    "# - model.parameters()获取模型中所有需要优化的参数\n",
    "# - lr=learning_rate设置学习率 : 控制每次参数更新的步长，太大可能导致震荡，太小会导致收敛慢\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0df25",
   "metadata": {},
   "source": [
    "# training - 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec0d291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.2888\n",
      "Epoch [1/10], Step [200/600], Loss: 0.1070\n",
      "Epoch [1/10], Step [300/600], Loss: 0.2887\n",
      "Epoch [1/10], Step [400/600], Loss: 0.0895\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1063\n",
      "Epoch [1/10], Step [600/600], Loss: 0.2541\n",
      "Epoch [2/10], Step [100/600], Loss: 0.1433\n",
      "Epoch [2/10], Step [200/600], Loss: 0.0451\n",
      "Epoch [2/10], Step [300/600], Loss: 0.0750\n",
      "Epoch [2/10], Step [400/600], Loss: 0.1178\n",
      "Epoch [2/10], Step [500/600], Loss: 0.1866\n",
      "Epoch [2/10], Step [600/600], Loss: 0.0323\n",
      "Epoch [3/10], Step [100/600], Loss: 0.0524\n",
      "Epoch [3/10], Step [200/600], Loss: 0.1047\n",
      "Epoch [3/10], Step [300/600], Loss: 0.1513\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0445\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0246\n",
      "Epoch [3/10], Step [600/600], Loss: 0.0830\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0213\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0144\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0285\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0438\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0154\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0900\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0165\n",
      "Epoch [5/10], Step [200/600], Loss: 0.1263\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0222\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0336\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0190\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0446\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0036\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0775\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0698\n",
      "Epoch [6/10], Step [400/600], Loss: 0.0082\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0514\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0168\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0039\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0037\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0159\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0213\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0282\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0084\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0014\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0263\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0154\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0057\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0131\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0003\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0047\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0009\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0146\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0197\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0283\n",
      "Epoch [9/10], Step [600/600], Loss: 0.0167\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0113\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0082\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0055\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0316\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0057\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0195\n"
     ]
    }
   ],
   "source": [
    "# 训练网络\n",
    "# 定义训练轮数(epochs)：将整个数据集完整训练10次\n",
    "# 为什么需要多轮训练:\n",
    "# 1. 单轮训练可能不足以让模型充分学习数据中的特征和规律\n",
    "# 2. 每轮训练会以不同的随机顺序遍历数据，增加模型见到不同数据组合的机会\n",
    "# 3. 多轮训练让模型参数能够逐步优化，达到更好的收敛效果\n",
    "# 4. 通过观察多轮训练的loss变化，可以判断模型是否过拟合或欠拟合\n",
    "num_epochs = 10  # 定义训练轮数：将整个数据集完整训练10次\n",
    "\n",
    "# 外层循环：遍历每个epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # 内层循环：遍历训练数据加载器中的每个批次\n",
    "    # images: 一个批次的图像数据\n",
    "    # labels: 对应的标签\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 预处理：将图像数据展平成一维向量 :  -1表示自动计算这个维度，28*28是MNIST图像的像素数\n",
    "        images = images.reshape(-1, 28 * 28)\n",
    "        \n",
    "        # 前向传播：将输入数据送入模型，得到预测结果\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 计算损失：使用交叉熵损失函数计算预测值和真实标签之间的差异\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播前先清零梯度 : 防止梯度累加，确保每次更新参数时只考虑当前批次的梯度\n",
    "        # 清零梯度是因为PyTorch默认会累积梯度\n",
    "        # 每次调用loss.backward()时，梯度会被累加到之前的梯度上\n",
    "        # 这会导致梯度计算错误，因此在每次反向传播前需要清零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 反向传播：计算损失函数对每个参数的梯度\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数：使用优化器根据计算得到的梯度更新模型参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 每训练100个批次打印一次训练信息\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], ' # 当前轮数/总轮数\n",
    "                  f'Step [{i+1}/{len(train_loader)}], ' # 当前批次/总批次数\n",
    "                  f'Loss: {loss.item():.4f}') # 当前批次的损失值，保留4位小数\n",
    "# 每个批次100张图像,一共有6w张图像,所以每轮需要600批次,才能训练完\n",
    "# 可以看到损失函数值逐渐在减小(也存在震荡,但是整体是减小的)，说明模型在学习数据的规律"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ffc84",
   "metadata": {},
   "source": [
    "# test - 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c0019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.16 %\n"
     ]
    }
   ],
   "source": [
    "# 测试网络 : 在测试阶段,不需要计算梯度,使用torch.no_grad()可以减少内存使用和加快计算速度\n",
    "with torch.no_grad():  \n",
    "    correct = 0  \n",
    "    total = 0    \n",
    "    # 从 test_loader中循环读取测试数据(图像和对应标签)\n",
    "    for images, labels in test_loader:\n",
    "        # 预处理：将图像数据(原来是28*28的二维)展平成一维向量 \n",
    "        # -1表示自动计算这个维度，28*28是MNIST图像的像素数\n",
    "        images = images.reshape(-1, 28 * 28)  \n",
    "        # 将数据送给网络进行前向传播,得到预测结果\n",
    "        outputs = model(images) \n",
    "        # 取出最大值对应的索引  即预测值\n",
    "        # 获取预测结果\n",
    "        # torch.max(outputs.data, 1)返回两个值:\n",
    "        # 1. 每一行中最大的值 (在这里不需要,用_占位)\n",
    "        # 2. 最大值对应的索引 (即模型预测的类别)\n",
    "        # dim=1表示在第1维度上取最大值(即每行)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)  \n",
    "        # 累加当前批次的样本总数\n",
    "        total += labels.size(0) \n",
    "        # 预测值与labels值比对 获取预测正确的数量\n",
    "        correct += (predicted == labels).sum().item()  \n",
    "    # 打印最终的准确率\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec351cb8",
   "metadata": {},
   "source": [
    "# save -保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1f14822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"mnist_mlp_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
